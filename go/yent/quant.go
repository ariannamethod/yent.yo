package yent

// quant.go — Q4_0 dequantization and quantized matrix operations
//
// GGML Q4_0 format:
//   Block of 32 elements = 18 bytes:
//     - 2 bytes: float16 scale factor (d)
//     - 16 bytes: 32 x 4-bit unsigned values packed in pairs
//   Each 4-bit value is unsigned [0..15], subtract 8 to get signed [-8..7]
//   Dequantized value = (q - 8) * d
//
// Memory layout per block:
//   [d_fp16] [q0q1] [q2q3] ... [q30q31]
//    2 bytes   1      1    ...    1     = 18 bytes total

import (
	"encoding/binary"
	"math"
	"runtime"
	"sync"
)

// Number of goroutines for parallel matmul
var numWorkers = runtime.NumCPU()

const q4BlockSize = 32   // elements per Q4_0 block
const q4BytesPerBlock = 18 // 2 (scale) + 16 (data)

// DequantQ4_0Block dequantizes a single Q4_0 block (32 values) into out
func DequantQ4_0Block(block []byte, out []float32) {
	// First 2 bytes = fp16 scale
	d := half2float(binary.LittleEndian.Uint16(block[0:2]))

	// Next 16 bytes = 32 x 4-bit values
	// GGML layout: low nibbles → positions 0..15, high nibbles → positions 16..31
	for j := 0; j < 16; j++ {
		b := block[2+j]
		v0 := int(b&0x0F) - 8
		v1 := int(b>>4) - 8
		out[j] = float32(v0) * d
		out[j+16] = float32(v1) * d
	}
}

// DequantQ4_0 dequantizes a full Q4_0 tensor into float32
func DequantQ4_0(data []byte, n int) []float32 {
	out := make([]float32, n)
	nblocks := n / q4BlockSize
	for i := 0; i < nblocks; i++ {
		off := i * q4BytesPerBlock
		DequantQ4_0Block(data[off:off+q4BytesPerBlock], out[i*q4BlockSize:])
	}
	return out
}

// MatMulQ4_0 computes out[rows] = W_q4[rows, cols] @ x[cols]
// W is stored as Q4_0 blocks in row-major order
// Parallelized across rows using goroutines (matches arianna.go pattern)
func MatMulQ4_0(out []float32, w []byte, x []float32, rows, cols int) {
	blocksPerRow := cols / q4BlockSize
	bytesPerRow := blocksPerRow * q4BytesPerBlock

	if rows < numWorkers*4 {
		// Small matrix — single thread
		matMulQ4_0Range(out, w, x, 0, rows, blocksPerRow, bytesPerRow)
		return
	}

	var wg sync.WaitGroup
	chunkSize := (rows + numWorkers - 1) / numWorkers

	for worker := 0; worker < numWorkers; worker++ {
		start := worker * chunkSize
		end := start + chunkSize
		if end > rows {
			end = rows
		}
		if start >= end {
			break
		}
		wg.Add(1)
		go func(s, e int) {
			matMulQ4_0Range(out, w, x, s, e, blocksPerRow, bytesPerRow)
			wg.Done()
		}(start, end)
	}
	wg.Wait()
}

func matMulQ4_0Range(out []float32, w []byte, x []float32, start, end, blocksPerRow, bytesPerRow int) {
	for i := start; i < end; i++ {
		rowOff := i * bytesPerRow
		sum := float32(0)

		for b := 0; b < blocksPerRow; b++ {
			blockOff := rowOff + b*q4BytesPerBlock
			d := half2float(binary.LittleEndian.Uint16(w[blockOff : blockOff+2]))

			xOff := b * q4BlockSize
			blockData := w[blockOff+2 : blockOff+q4BytesPerBlock]

			var dot float32
			for j := 0; j < 16; j++ {
				bv := blockData[j]
				v0 := float32(int(bv&0x0F) - 8)
				v1 := float32(int(bv>>4) - 8)
				dot += v0*x[xOff+j] + v1*x[xOff+j+16]
			}
			sum += dot * d
		}
		out[i] = sum
	}
}

// ============================================================
// Q8_0 dequantization (GGML type 8)
// ============================================================
//
// Q8_0: 8-bit quantization, 32 elements per block = 34 bytes:
//   - 2 bytes: float16 scale factor (d)
//   - 32 bytes: 32 x int8 values
//   Dequantized value = q * d

const q8BlockSize = 32
const q8BytesPerBlock = 34 // 2 (scale) + 32 (data)

// DequantQ8_0Block dequantizes a single Q8_0 block (32 values) into out
func DequantQ8_0Block(block []byte, out []float32) {
	d := half2float(binary.LittleEndian.Uint16(block[0:2]))
	for j := 0; j < 32; j++ {
		out[j] = float32(int8(block[2+j])) * d
	}
}

// DequantQ8_0 dequantizes a full Q8_0 tensor into float32
func DequantQ8_0(data []byte, n int) []float32 {
	out := make([]float32, n)
	nblocks := n / q8BlockSize
	for i := 0; i < nblocks; i++ {
		off := i * q8BytesPerBlock
		DequantQ8_0Block(data[off:off+q8BytesPerBlock], out[i*q8BlockSize:])
	}
	return out
}

// MatMulQ8_0 computes out[rows] = W_q8[rows, cols] @ x[cols]
// Parallelized across rows using goroutines
func MatMulQ8_0(out []float32, w []byte, x []float32, rows, cols int) {
	blocksPerRow := cols / q8BlockSize
	bytesPerRow := blocksPerRow * q8BytesPerBlock

	if rows < numWorkers*4 {
		matMulQ8_0Range(out, w, x, 0, rows, blocksPerRow, bytesPerRow)
		return
	}

	var wg sync.WaitGroup
	chunkSize := (rows + numWorkers - 1) / numWorkers

	for worker := 0; worker < numWorkers; worker++ {
		start := worker * chunkSize
		end := start + chunkSize
		if end > rows {
			end = rows
		}
		if start >= end {
			break
		}
		wg.Add(1)
		go func(s, e int) {
			matMulQ8_0Range(out, w, x, s, e, blocksPerRow, bytesPerRow)
			wg.Done()
		}(start, end)
	}
	wg.Wait()
}

func matMulQ8_0Range(out []float32, w []byte, x []float32, start, end, blocksPerRow, bytesPerRow int) {
	for i := start; i < end; i++ {
		rowOff := i * bytesPerRow
		sum := float32(0)

		for b := 0; b < blocksPerRow; b++ {
			blockOff := rowOff + b*q8BytesPerBlock
			d := half2float(binary.LittleEndian.Uint16(w[blockOff : blockOff+2]))

			xOff := b * q8BlockSize
			var dot float32
			for j := 0; j < 32; j++ {
				dot += float32(int8(w[blockOff+2+j])) * x[xOff+j]
			}
			sum += dot * d
		}
		out[i] = sum
	}
}

// EmbedLookupQ8_0 extracts one row from a Q8_0 embedding table
func EmbedLookupQ8_0(data []byte, token, dim int) []float32 {
	blocksPerRow := dim / q8BlockSize
	bytesPerRow := blocksPerRow * q8BytesPerBlock
	rowOff := token * bytesPerRow
	out := make([]float32, dim)

	for b := 0; b < blocksPerRow; b++ {
		blockOff := rowOff + b*q8BytesPerBlock
		DequantQ8_0Block(data[blockOff:blockOff+q8BytesPerBlock], out[b*q8BlockSize:])
	}
	return out
}

// ============================================================
// Q6_K dequantization (GGML type 14)
// ============================================================
//
// Q6_K: 6-bit k-quant, 256 elements per super-block = 210 bytes:
//   ql[128] — low 4 bits of each quant
//   qh[64]  — high 2 bits of each quant
//   scales[16] — int8 sub-block scales
//   d (fp16) — super-block scale factor
//
// Each element: 6-bit unsigned (0-63), subtract 32 for signed (-32 to +31)
// Dequantized = d * scales[sub_block] * (q6_val - 32)

const q6kBlockSize = 256
const q6kBytesPerBlock = 210

// DequantQ6_K dequantizes a full Q6_K tensor into float32
func DequantQ6_K(data []byte, n int) []float32 {
	out := make([]float32, n)
	nblocks := n / q6kBlockSize

	for i := 0; i < nblocks; i++ {
		blockOff := i * q6kBytesPerBlock
		ql := data[blockOff:]
		qh := data[blockOff+128:]
		scales := data[blockOff+192:]
		d := half2float(binary.LittleEndian.Uint16(data[blockOff+208 : blockOff+210]))

		outOff := i * q6kBlockSize

		// Process 128 elements at a time (2 passes for 256)
		for n128 := 0; n128 < 2; n128++ {
			qlP := ql[n128*64:]
			qhP := qh[n128*32:]
			scP := scales[n128*8:]
			yOff := outOff + n128*128

			for l := 0; l < 32; l++ {
				is := l / 16 // 0 for l=0..15, 1 for l=16..31
				q1 := int(qlP[l]&0x0F) | (int(qhP[l]>>0)&3)<<4
				q2 := int(qlP[l+32]&0x0F) | (int(qhP[l]>>2)&3)<<4
				q3 := int(qlP[l]>>4) | (int(qhP[l]>>4)&3)<<4
				q4 := int(qlP[l+32]>>4) | (int(qhP[l]>>6)&3)<<4

				out[yOff+l+0] = d * float32(int8(scP[is+0])) * float32(q1-32)
				out[yOff+l+32] = d * float32(int8(scP[is+2])) * float32(q2-32)
				out[yOff+l+64] = d * float32(int8(scP[is+4])) * float32(q3-32)
				out[yOff+l+96] = d * float32(int8(scP[is+6])) * float32(q4-32)
			}
		}
	}
	return out
}

// MatMulQ6_K computes out[rows] = W_q6k[rows, cols] @ x[cols]
// Parallelized across rows using goroutines
func MatMulQ6_K(out []float32, w []byte, x []float32, rows, cols int) {
	blocksPerRow := cols / q6kBlockSize
	bytesPerRow := blocksPerRow * q6kBytesPerBlock

	if rows < numWorkers*4 {
		matMulQ6_KRange(out, w, x, 0, rows, blocksPerRow, bytesPerRow)
		return
	}

	var wg sync.WaitGroup
	chunkSize := (rows + numWorkers - 1) / numWorkers

	for worker := 0; worker < numWorkers; worker++ {
		start := worker * chunkSize
		end := start + chunkSize
		if end > rows {
			end = rows
		}
		if start >= end {
			break
		}
		wg.Add(1)
		go func(s, e int) {
			matMulQ6_KRange(out, w, x, s, e, blocksPerRow, bytesPerRow)
			wg.Done()
		}(start, end)
	}
	wg.Wait()
}

func matMulQ6_KRange(out []float32, w []byte, x []float32, start, end, blocksPerRow, bytesPerRow int) {
	for r := start; r < end; r++ {
		rowOff := r * bytesPerRow
		sum := float32(0)

		for b := 0; b < blocksPerRow; b++ {
			blockOff := rowOff + b*q6kBytesPerBlock
			ql := w[blockOff:]
			qh := w[blockOff+128:]
			scales := w[blockOff+192:]
			d := half2float(binary.LittleEndian.Uint16(w[blockOff+208 : blockOff+210]))

			xOff := b * q6kBlockSize

			for n128 := 0; n128 < 2; n128++ {
				qlP := ql[n128*64:]
				qhP := qh[n128*32:]
				scP := scales[n128*8:]
				xBase := xOff + n128*128

				for l := 0; l < 32; l++ {
					is := l / 16 // 0 for l=0..15, 1 for l=16..31
					q1 := int(qlP[l]&0x0F) | (int(qhP[l]>>0)&3)<<4
					q2 := int(qlP[l+32]&0x0F) | (int(qhP[l]>>2)&3)<<4
					q3 := int(qlP[l]>>4) | (int(qhP[l]>>4)&3)<<4
					q4 := int(qlP[l+32]>>4) | (int(qhP[l]>>6)&3)<<4

					s0 := d * float32(int8(scP[is+0]))
					s2 := d * float32(int8(scP[is+2]))
					s4 := d * float32(int8(scP[is+4]))
					s6 := d * float32(int8(scP[is+6]))

					sum += s0 * float32(q1-32) * x[xBase+l+0]
					sum += s2 * float32(q2-32) * x[xBase+l+32]
					sum += s4 * float32(q3-32) * x[xBase+l+64]
					sum += s6 * float32(q4-32) * x[xBase+l+96]
				}
			}
		}
		out[r] = sum
	}
}

// ============================================================
// Q4_K dequantization (GGML type 12)
// ============================================================
//
// Q4_K: 4-bit k-quant, 256 elements per super-block = 144 bytes:
//   d (fp16)        — super-block scale
//   dmin (fp16)     — super-block minimum
//   scales[12]      — 8 sub-block scales + 8 mins, 6-bit packed
//   qs[128]         — 256 × 4-bit quantized values in pairs
//
// Sub-block scale/min extraction (get_scale_min_k4):
//   j < 4: sc = scales[j] & 63, m = scales[j+4] & 63
//   j >= 4: sc = (scales[j+4] & 0xF) | ((scales[j-4] >> 6) << 4)
//           m  = (scales[j+4] >> 4)   | ((scales[j]   >> 6) << 4)
//
// Dequant: y = d * sc * (q4_val) - dmin * m

const q4kBlockSize = 256
const q4kBytesPerBlock = 144

// getScaleMinK4 extracts sub-block scale and min from packed 12-byte scales array
func getScaleMinK4(j int, scales []byte) (sc, m uint8) {
	if j < 4 {
		sc = scales[j] & 63
		m = scales[j+4] & 63
	} else {
		sc = (scales[j+4] & 0x0F) | ((scales[j-4] >> 6) << 4)
		m = (scales[j+4] >> 4) | ((scales[j] >> 6) << 4)
	}
	return
}

// DequantQ4_KBlock dequantizes a single Q4_K super-block (256 values) into out
func DequantQ4_KBlock(block []byte, out []float32) {
	d := half2float(binary.LittleEndian.Uint16(block[0:2]))
	dmin := half2float(binary.LittleEndian.Uint16(block[2:4]))
	scales := block[4:16] // 12 bytes
	qs := block[16:]      // 128 bytes

	is := 0
	outIdx := 0
	qIdx := 0
	for j := 0; j < q4kBlockSize; j += 64 {
		sc0, m0 := getScaleMinK4(is, scales)
		d1 := d * float32(sc0)
		m1 := dmin * float32(m0)
		sc1, m1v := getScaleMinK4(is+1, scales)
		d2 := d * float32(sc1)
		m2 := dmin * float32(m1v)

		for l := 0; l < 32; l++ {
			out[outIdx+l] = d1*float32(qs[qIdx+l]&0x0F) - m1
		}
		for l := 0; l < 32; l++ {
			out[outIdx+32+l] = d2*float32(qs[qIdx+l]>>4) - m2
		}
		qIdx += 32
		outIdx += 64
		is += 2
	}
}

// DequantQ4_K dequantizes a full Q4_K tensor into float32
func DequantQ4_K(data []byte, n int) []float32 {
	out := make([]float32, n)
	nblocks := n / q4kBlockSize
	for i := 0; i < nblocks; i++ {
		off := i * q4kBytesPerBlock
		DequantQ4_KBlock(data[off:off+q4kBytesPerBlock], out[i*q4kBlockSize:])
	}
	return out
}

// MatMulQ4_K computes out[rows] = W_q4k[rows, cols] @ x[cols]
// Parallelized across rows using goroutines
func MatMulQ4_K(out []float32, w []byte, x []float32, rows, cols int) {
	blocksPerRow := cols / q4kBlockSize
	bytesPerRow := blocksPerRow * q4kBytesPerBlock

	if rows < numWorkers*4 {
		matMulQ4_KRange(out, w, x, 0, rows, blocksPerRow, bytesPerRow)
		return
	}

	var wg sync.WaitGroup
	chunkSize := (rows + numWorkers - 1) / numWorkers

	for worker := 0; worker < numWorkers; worker++ {
		start := worker * chunkSize
		end := start + chunkSize
		if end > rows {
			end = rows
		}
		if start >= end {
			break
		}
		wg.Add(1)
		go func(s, e int) {
			matMulQ4_KRange(out, w, x, s, e, blocksPerRow, bytesPerRow)
			wg.Done()
		}(start, end)
	}
	wg.Wait()
}

func matMulQ4_KRange(out []float32, w []byte, x []float32, start, end, blocksPerRow, bytesPerRow int) {
	for r := start; r < end; r++ {
		rowOff := r * bytesPerRow
		sum := float32(0)

		for b := 0; b < blocksPerRow; b++ {
			blockOff := rowOff + b*q4kBytesPerBlock
			d := half2float(binary.LittleEndian.Uint16(w[blockOff : blockOff+2]))
			dmin := half2float(binary.LittleEndian.Uint16(w[blockOff+2 : blockOff+4]))
			scales := w[blockOff+4 : blockOff+16]
			qs := w[blockOff+16:]

			xOff := b * q4kBlockSize
			is := 0
			qIdx := 0

			for j := 0; j < q4kBlockSize; j += 64 {
				sc0, m0 := getScaleMinK4(is, scales)
				d1 := d * float32(sc0)
				m1 := dmin * float32(m0)
				sc1, m1v := getScaleMinK4(is+1, scales)
				d2 := d * float32(sc1)
				m2 := dmin * float32(m1v)

				for l := 0; l < 32; l++ {
					sum += (d1*float32(qs[qIdx+l]&0x0F) - m1) * x[xOff+j+l]
				}
				for l := 0; l < 32; l++ {
					sum += (d2*float32(qs[qIdx+l]>>4) - m2) * x[xOff+j+32+l]
				}
				qIdx += 32
				is += 2
			}
		}
		out[r] = sum
	}
}

// ============================================================
// Q5_0 dequantization (GGML type 6)
// ============================================================
//
// Q5_0: 5-bit quantization, 32 elements per block = 22 bytes:
//   d (fp16)     — scale factor
//   qh[4]        — high bits (bit 4) for each of 32 elements, packed as uint32
//   qs[16]       — low 4 bits, packed in pairs (like Q4_0)
//
// 5-bit value [0..31], subtract 16 for signed [-16..15]
// Dequantized value = (q5 - 16) * d

const q50BlockSize = 32
const q50BytesPerBlock = 22

// DequantQ5_0Block dequantizes a single Q5_0 block (32 values) into out
func DequantQ5_0Block(block []byte, out []float32) {
	d := half2float(binary.LittleEndian.Uint16(block[0:2]))
	qh := binary.LittleEndian.Uint32(block[2:6])
	qs := block[6:22]

	for j := 0; j < 16; j++ {
		// Low nibble = position j, high nibble = position j+16
		lo := int(qs[j] & 0x0F)
		hi := int(qs[j] >> 4)

		// Bit 4 from qh
		hbit0 := int((qh >> uint(j)) & 1)
		hbit1 := int((qh >> uint(j+16)) & 1)

		q0 := lo | (hbit0 << 4) // 5-bit value
		q1 := hi | (hbit1 << 4) // 5-bit value

		out[j] = float32(q0-16) * d
		out[j+16] = float32(q1-16) * d
	}
}

// DequantQ5_0 dequantizes a full Q5_0 tensor into float32
func DequantQ5_0(data []byte, n int) []float32 {
	out := make([]float32, n)
	nblocks := n / q50BlockSize
	for i := 0; i < nblocks; i++ {
		off := i * q50BytesPerBlock
		DequantQ5_0Block(data[off:off+q50BytesPerBlock], out[i*q50BlockSize:])
	}
	return out
}

// MatMulQ5_0 computes out[rows] = W_q50[rows, cols] @ x[cols]
// Parallelized across rows using goroutines
func MatMulQ5_0(out []float32, w []byte, x []float32, rows, cols int) {
	blocksPerRow := cols / q50BlockSize
	bytesPerRow := blocksPerRow * q50BytesPerBlock

	if rows < numWorkers*4 {
		matMulQ5_0Range(out, w, x, 0, rows, blocksPerRow, bytesPerRow)
		return
	}

	var wg sync.WaitGroup
	chunkSize := (rows + numWorkers - 1) / numWorkers

	for worker := 0; worker < numWorkers; worker++ {
		start := worker * chunkSize
		end := start + chunkSize
		if end > rows {
			end = rows
		}
		if start >= end {
			break
		}
		wg.Add(1)
		go func(s, e int) {
			matMulQ5_0Range(out, w, x, s, e, blocksPerRow, bytesPerRow)
			wg.Done()
		}(start, end)
	}
	wg.Wait()
}

func matMulQ5_0Range(out []float32, w []byte, x []float32, start, end, blocksPerRow, bytesPerRow int) {
	for r := start; r < end; r++ {
		rowOff := r * bytesPerRow
		sum := float32(0)

		for b := 0; b < blocksPerRow; b++ {
			blockOff := rowOff + b*q50BytesPerBlock
			d := half2float(binary.LittleEndian.Uint16(w[blockOff : blockOff+2]))
			qh := binary.LittleEndian.Uint32(w[blockOff+2 : blockOff+6])
			qs := w[blockOff+6:]

			xOff := b * q50BlockSize

			for j := 0; j < 16; j++ {
				lo := int(qs[j] & 0x0F)
				hi := int(qs[j] >> 4)
				hbit0 := int((qh >> uint(j)) & 1)
				hbit1 := int((qh >> uint(j+16)) & 1)
				q0 := lo | (hbit0 << 4)
				q1 := hi | (hbit1 << 4)
				sum += float32(q0-16) * d * x[xOff+j]
				sum += float32(q1-16) * d * x[xOff+j+16]
			}
		}
		out[r] = sum
	}
}

// MatMulF32 computes out[rows] = W_f32[rows, cols] @ x[cols]
// Parallelized across rows using goroutines
func MatMulF32(out []float32, w []float32, x []float32, rows, cols int) {
	if rows < numWorkers*4 {
		matMulF32Range(out, w, x, 0, rows, cols)
		return
	}

	var wg sync.WaitGroup
	chunkSize := (rows + numWorkers - 1) / numWorkers

	for worker := 0; worker < numWorkers; worker++ {
		start := worker * chunkSize
		end := start + chunkSize
		if end > rows {
			end = rows
		}
		if start >= end {
			break
		}
		wg.Add(1)
		go func(s, e int) {
			matMulF32Range(out, w, x, s, e, cols)
			wg.Done()
		}(start, end)
	}
	wg.Wait()
}

func matMulF32Range(out []float32, w []float32, x []float32, start, end, cols int) {
	for i := start; i < end; i++ {
		sum := float32(0)
		off := i * cols
		for j := 0; j < cols; j++ {
			sum += w[off+j] * x[j]
		}
		out[i] = sum
	}
}

// MatMulF16 computes out[rows] = W_f16[rows, cols] @ x[cols]
// w is raw bytes of float16 values
// Parallelized across rows using goroutines
func MatMulF16(out []float32, w []byte, x []float32, rows, cols int) {
	if rows < numWorkers*4 {
		matMulF16Range(out, w, x, 0, rows, cols)
		return
	}

	var wg sync.WaitGroup
	chunkSize := (rows + numWorkers - 1) / numWorkers

	for worker := 0; worker < numWorkers; worker++ {
		start := worker * chunkSize
		end := start + chunkSize
		if end > rows {
			end = rows
		}
		if start >= end {
			break
		}
		wg.Add(1)
		go func(s, e int) {
			matMulF16Range(out, w, x, s, e, cols)
			wg.Done()
		}(start, end)
	}
	wg.Wait()
}

func matMulF16Range(out []float32, w []byte, x []float32, start, end, cols int) {
	for i := start; i < end; i++ {
		sum := float32(0)
		rowOff := i * cols * 2
		for j := 0; j < cols; j++ {
			wv := half2float(binary.LittleEndian.Uint16(w[rowOff+j*2 : rowOff+j*2+2]))
			sum += wv * x[j]
		}
		out[i] = sum
	}
}

// EmbedLookupQ4_0 extracts one row from a Q4_0 embedding table
func EmbedLookupQ4_0(data []byte, token, dim int) []float32 {
	blocksPerRow := dim / q4BlockSize
	bytesPerRow := blocksPerRow * q4BytesPerBlock
	rowOff := token * bytesPerRow
	out := make([]float32, dim)

	for b := 0; b < blocksPerRow; b++ {
		blockOff := rowOff + b*q4BytesPerBlock
		DequantQ4_0Block(data[blockOff:blockOff+q4BytesPerBlock], out[b*q4BlockSize:])
	}
	return out
}

// EmbedLookupF32 extracts one row from an F32 embedding table
func EmbedLookupF32(data []float32, token, dim int) []float32 {
	out := make([]float32, dim)
	copy(out, data[token*dim:(token+1)*dim])
	return out
}

// RMSNorm applies RMS normalization in-place
func RMSNorm(x []float32, w []float32, eps float32) {
	n := len(x)
	var ss float64
	for i := 0; i < n; i++ {
		ss += float64(x[i]) * float64(x[i])
	}
	inv := float32(1.0 / math.Sqrt(ss/float64(n)+float64(eps)))
	for i := 0; i < n; i++ {
		x[i] = x[i] * inv * w[i]
	}
}

// RMSNormBare applies RMS normalization in-place WITHOUT learnable weights.
// Used for QK-norm in nanollama (parameterless RMSNorm).
func RMSNormBare(x []float32, eps float32) {
	n := len(x)
	var ss float64
	for i := 0; i < n; i++ {
		ss += float64(x[i]) * float64(x[i])
	}
	inv := float32(1.0 / math.Sqrt(ss/float64(n)+float64(eps)))
	for i := 0; i < n; i++ {
		x[i] *= inv
	}
}

// RMSNormInto applies RMS normalization: out = norm(x) * w
func RMSNormInto(out, x, w []float32, eps float32) {
	n := len(x)
	var ss float64
	for i := 0; i < n; i++ {
		ss += float64(x[i]) * float64(x[i])
	}
	inv := float32(1.0 / math.Sqrt(ss/float64(n)+float64(eps)))
	for i := 0; i < n; i++ {
		out[i] = x[i] * inv * w[i]
	}
}

// Softmax computes softmax in-place over x[0:n]
func Softmax(x []float32, n int) {
	max := x[0]
	for i := 1; i < n; i++ {
		if x[i] > max {
			max = x[i]
		}
	}
	var sum float32
	for i := 0; i < n; i++ {
		x[i] = float32(math.Exp(float64(x[i] - max)))
		sum += x[i]
	}
	inv := float32(1.0) / sum
	for i := 0; i < n; i++ {
		x[i] *= inv
	}
}

// SiLU activation: x * sigmoid(x)
func SiLU(x float32) float32 {
	return x / (1.0 + float32(math.Exp(float64(-x))))
}

// blendQ8_0 blends two Q8_0 tensors in place: dst = alpha*dst + (1-alpha)*src
// Both tensors must be same size Q8_0 format. Parallelized for large tensors.
func blendQ8_0(dst, src []byte, alpha float32) {
	if len(dst) != len(src) {
		return
	}

	nblocks := len(dst) / q8BytesPerBlock
	beta := 1.0 - alpha

	// Parallel blend for large tensors
	var wg sync.WaitGroup
	chunkSize := (nblocks + numWorkers - 1) / numWorkers

	for w := 0; w < numWorkers; w++ {
		start := w * chunkSize
		end := start + chunkSize
		if end > nblocks {
			end = nblocks
		}
		if start >= end {
			break
		}
		wg.Add(1)
		go func(s, e int) {
			dstVals := make([]float32, 32)
			srcVals := make([]float32, 32)

			for i := s; i < e; i++ {
				off := i * q8BytesPerBlock

				DequantQ8_0Block(dst[off:off+q8BytesPerBlock], dstVals)
				DequantQ8_0Block(src[off:off+q8BytesPerBlock], srcVals)

				var maxAbs float32
				for j := 0; j < 32; j++ {
					dstVals[j] = alpha*dstVals[j] + beta*srcVals[j]
					if abs := float32(math.Abs(float64(dstVals[j]))); abs > maxAbs {
						maxAbs = abs
					}
				}

				var scale float32
				if maxAbs > 0 {
					scale = maxAbs / 127.0
				}

				scaleFp16 := float2half(scale)
				dst[off] = byte(scaleFp16)
				dst[off+1] = byte(scaleFp16 >> 8)

				invScale := float32(0)
				if scale > 0 {
					invScale = 1.0 / scale
				}
				for j := 0; j < 32; j++ {
					q := int(math.Round(float64(dstVals[j] * invScale)))
					if q > 127 {
						q = 127
					} else if q < -128 {
						q = -128
					}
					dst[off+2+j] = byte(int8(q))
				}
			}
			wg.Done()
		}(start, end)
	}
	wg.Wait()
}

// float2half converts float32 to float16 (fp16)
func float2half(f float32) uint16 {
	bits := math.Float32bits(f)
	sign := (bits >> 16) & 0x8000
	exp := int((bits>>23)&0xFF) - 127 + 15
	mant := bits & 0x7FFFFF

	if exp <= 0 {
		return uint16(sign) // underflow to zero
	}
	if exp >= 31 {
		return uint16(sign | 0x7C00) // overflow to inf
	}
	return uint16(sign | uint32(exp<<10) | (mant >> 13))
}
